DATABRICKS_QUIZ_ES = [
    {
        "exam": 1,
        "id": "q01_delta_optimize_compaction",
        "question_es": "¿Qué comando compacta muchos archivos pequeños de una tabla Delta en archivos más grandes?",
        "options": ["OPTIMIZE", "VACUUM", "COMPACT", "ZORDER BY"],
        "answer": "OPTIMIZE",
        "explanation_es": "OPTIMIZE reescribe/compacta small files. VACUUM limpia archivos antiguos; ZORDER ordena; COMPACT no existe como comando.",
    },
    {
        "exam": 1,
        "id": "q02_dlt_source_parquet_gcs",
        "question_es": "DLT: ¿Qué opción crea una tabla leyendo Parquet desde GCS (gs://...)?",
        "options": [
            '@dlt.table\ndef iot_data():\n    return spark.read.parquet("gs://data/iot/raw/")',
            '@dlt.table\ndef iot_data():\n    return spark.write.parquet("gs://data/iot/raw/")',
            '@dlt.table\ndef iot_data():\n    return spark.parquet.load("gs://data/iot/raw/")',
            '@dlt.view\ndef iot_data():\n    return spark.read.format("parquet").load("gs://data/iot/raw/")',
        ],
        "answer": '@dlt.table\ndef iot_data():\n    return spark.read.parquet("gs://data/iot/raw/")',
        "explanation_es": "@dlt.table + spark.read.parquet(...) devuelve un DataFrame fuente. write no lee, y @dlt.view crea vista, no tabla.",
    },
    {
        "exam": 1,
        "id": "q03_sql_udf_create_function",
        "question_es": "SQL: ¿Cuál es la sintaxis correcta para crear una UDF permanente?",
        "options": [
            "CREATE UDF plus_one(value INTEGER) RETURN value +1;",
            "CREATE UDF plus_one(value INTEGER) RETURNS INTEGER RETURN value +1;",
            "CREATE FUNCTION plus_one(value INTEGER) RETURNS INTEGER RETURN value +1;",
            "CREATE FUNCTION plus_one(value INTEGER) RETURN value +1",
        ],
        "answer": "CREATE FUNCTION plus_one(value INTEGER) RETURNS INTEGER RETURN value +1;",
        "explanation_es": "En Databricks/Spark SQL se usa CREATE FUNCTION y hay que indicar RETURNS <tipo>.",
    },
    {
        "exam": 1,
        "id": "q04_lakehouse_federation_purpose",
        "question_es": "Lakehouse Federation: ¿para qué sirve principalmente?",
        "options": [
            "To create backups of data stored in Databricks",
            "To migrate all data into Databricks for centralized processing",
            "To optimize storage costs by compressing data",
            "To enable direct querying across multiple data sources without duplicating data",
        ],
        "answer": "To enable direct querying across multiple data sources without duplicating data",
        "explanation_es": "Permite consultar fuentes externas sin mover/replicar datos (virtualización).",
    },
    {
        "exam": 1,
        "id": "q05_data_skew_not_solution",
        "question_es": "Join con skew (pocas claves muy frecuentes): ¿qué NO es buena solución?",
        "options": [
            "Separate processing of skewed keys by handling high-frequency users in a dedicated job.",
            "Broadcast the skewed keys to all worker nodes to avoid shuffle during the join.",
            "Use salting by appending a random prefix to skewed user_id values to distribute the load across partitions.",
            "Repartition the clickstream dataset to increase the number of partitions before the join.",
        ],
        "answer": "Broadcast the skewed keys to all worker nodes to avoid shuffle during the join.",
        "explanation_es": "Broadcast va bien con tablas pequeñas; con claves sesgadas y muchos registros puede reventar memoria (OOM).",
    },
    {
        "exam": 1,
        "id": "q06_databricks_sql_compute",
        "question_es": "En Databricks SQL, ¿qué recurso de cómputo se usa?",
        "options": ["SQL warehouses", "Single-node clusters", "Multi-nodes clusters", "SQL engines"],
        "answer": "SQL warehouses",
        "explanation_es": "Databricks SQL usa SQL Warehouses (antes SQL Endpoints).",
    },
    {
        "exam": 1,
        "id": "q07_dabs_config_file",
        "question_es": "DABs: ¿cómo se llama y qué formato tiene el fichero de configuración?",
        "options": [
            "A YAML file named bundle-config.yml with fields for environment variables and user roles",
            "A YAML file named databricks.yml that defines the bundle's structure, including targets, resources, and configurations.",
            "A JSON file named databricks_asset.json containing cluster definitions and job schedules",
            "A XML file named asset_bundle.xml specifying the workspace path, job IDs, and compute specifications",
        ],
        "answer": "A YAML file named databricks.yml that defines the bundle's structure, including targets, resources, and configurations.",
        "explanation_es": "El archivo se llama databricks.yml (YAML) y define targets/resources/config.",
    },
    {
        "exam": 1,
        "id": "q08_vacuum_default_retention",
        "question_es": "VACUUM: ¿cuál es la retención por defecto?",
        "options": ["365 days", "0 days", "30 days", "7 days"],
        "answer": "7 days",
        "explanation_es": "Por seguridad (time travel / lectores lentos), default 7 días.",
    },
    {
        "exam": 1,
        "id": "q09_sql_array_filter",
        "question_es": "SQL: en un array de structs, ¿qué función filtra elementos con total_courses < 3?",
        "options": [
            "TRANSFORM (students, i -> i.total_courses < 3)",
            "TRANSFORM (students, total_courses < 3)",
            "FILTER (students, i -> i.total_courses < 3)",
            "FILTER (students, total_courses < 3)",
        ],
        "answer": "FILTER (students, i -> i.total_courses < 3)",
        "explanation_es": "FILTER devuelve un subconjunto. TRANSFORM mapea y mantiene tamaño.",
    },
    {
        "exam": 1,
        "id": "q10_medallion_gold_consumers",
        "question_es": "Arquitectura Medallion: ¿quién consume tablas Gold normalmente?",
        "options": ["Bronze tables", "Dashboards", "Silver tables", "Auto loader"],
        "answer": "Dashboards",
        "explanation_es": "Gold está lista para negocio: BI/reporting/dashboards.",
    },
    {
        "exam": 1,
        "id": "q11_dlt_cdc_apply_changes",
        "question_es": "DLT CDC (inserts/updates/deletes con timestamp): ¿qué comando usarías?",
        "options": ["UPDATE", "MERGE INTO", "COPY INTO", "APPLY CHANGES INTO"],
        "answer": "APPLY CHANGES INTO",
        "explanation_es": "APPLY CHANGES INTO está pensado para CDC en DLT (orden, dedupe, deletes).",
    },
    {
        "exam": 1,
        "id": "q12_variable_explorer",
        "question_es": "¿Qué herramienta permite inspeccionar variables/tipos en tiempo real en notebooks?",
        "options": ["display()", "Notebook Variable Explorer", "print()", "dbutils.variables.summary()"],
        "answer": "Notebook Variable Explorer",
        "explanation_es": "El Variable Explorer lista variables activas con tipo y valor.",
    },
    {
        "exam": 1,
        "id": "q13_cicd_dabs_best_practice",
        "question_es": "CI/CD: código en Git y despliegue fiable entre entornos. ¿Mejor opción?",
        "options": [
            "Leverage Notebook built-in version history for source control, and create jobs using Databricks UI",
            "Store the source code in Git Folders, and deploy jobs using Databricks REST API",
            "Leverage Notebook built-in version history for source control, and deploy jobs using serverless notebook's Environment",
            "Store the source code in Git folders, and deploy jobs using Databricks Asset Bundles (DAB)",
        ],
        "answer": "Store the source code in Git folders, and deploy jobs using Databricks Asset Bundles (DAB)",
        "explanation_es": "Best practice: Git + Databricks Asset Bundles para IaC y despliegue por entornos.",
    },
    {
        "exam": 1,
        "id": "q14_serverless_language_limits",
        "question_es": "Serverless: ¿por qué falla ejecutar Scala?",
        "options": [
            "Scala is not a supported language on the Databricks platform",
            "Scala requires a classic compute with runtime version 16.4 LTS or above",
            "The serverless compute supports only SQL",
            "The serverless compute supports only SQL and Python",
        ],
        "answer": "The serverless compute supports only SQL and Python",
        "explanation_es": "En ese contexto serverless, solo SQL y Python (Scala/R requieren classic).",
    },
    {
        "exam": 1,
        "id": "q15_medallion_bronze_ingestion",
        "question_es": "Auto Loader (cloudFiles) escribiendo en 'uncleanedOrders': ¿qué capa Medallion es?",
        "options": [
            "The query is performing data transfer from a Gold table into a production application",
            "The query is performing raw data ingestion into a Bronze table",
            "The query is performing a hop from Silver table to a Gold table",
            "The query is performing a hop from a Bronze table to a Silver table",
        ],
        "answer": "The query is performing raw data ingestion into a Bronze table",
        "explanation_es": "Ingesta raw → Bronze (datos sin limpiar).",
    },
    {
        "exam": 1,
        "id": "q16_delta_not_true_xml",
        "question_es": "Delta Lake: ¿qué afirmación es falsa?",
        "options": [
            "Delta Lake provides scalable data and metadata handling",
            "Delta Lake builds upon standard data formats: Parquet + XML",
            "Delta Lake provides ACID transaction guarantees",
            "Delta Lake provides audit history and time travel",
        ],
        "answer": "Delta Lake builds upon standard data formats: Parquet + XML",
        "explanation_es": "Delta usa Parquet + _delta_log (JSON), no XML.",
    },
    {
        "exam": 1,
        "id": "q17_pyspark_groupby_agg",
        "question_es": "PySpark: para max/min por customer_id, ¿qué va antes de agg()?",
        "options": ["window", "withColumn", "groupBy", "select"],
        "answer": "groupBy",
        "explanation_es": "Agregación por grupo: df.groupBy(...).agg(...).",
    },
    {
        "exam": 1,
        "id": "q18_notebook_interactive_debugger",
        "question_es": "Notebook: pausar, step-by-step y ver variables en ejecución. ¿Qué usar?",
        "options": ["%debug magic command", "%run magic command", "Databricks Web Terminal", "Notebook Interactive Debugger"],
        "answer": "Notebook Interactive Debugger",
        "explanation_es": "Depurador nativo con breakpoints y ejecución paso a paso.",
    },
    {
        "exam": 1,
        "id": "q19_serverless_job_compute_hourly",
        "question_es": "Job Python hourly, poco volumen, arranque rápido sin pre-warm: ¿qué compute?",
        "options": ["Serverless SQL Warehouse", "Classic All-purpose Cluster", "Classic Job Cluster", "Serverless Job Compute"],
        "answer": "Serverless Job Compute",
        "explanation_es": "Ideal para jobs pequeños y frecuentes: arranque rápido y eficiente.",
    },
    {
        "exam": 1,
        "id": "q20_magic_python_in_sql_notebook",
        "question_es": "Notebook SQL: ¿cómo ejecutas Python en una celda?",
        "options": [
            "This is not possible! They need to change the default language of the notebook to Python",
            "They can add %python at the start of a cell.",
            "They can add %language magic command at the start of a cell to force language detection.",
            "They need first to import the python module in a cell",
        ],
        "answer": "They can add %python at the start of a cell.",
        "explanation_es": "Notebooks políglotas: %python al inicio de la celda.",
    },
    {
        "exam": 1,
        "id": "q21_dlt_continuous_production",
        "question_es": "DLT: corre hasta que lo paras manualmente y al parar termina el compute. ¿Modo?",
        "options": [
            "The DLT pipeline executes in Triggered Pipeline mode under Production mode.",
            "The DLT pipeline executes in Continuous Pipeline mode under Production mode.",
            "The DLT pipeline executes in Triggered Pipeline mode under Development mode.",
            "The DLT pipeline executes in Continuous Pipeline mode under Development mode.",
        ],
        "answer": "The DLT pipeline executes in Continuous Pipeline mode under Production mode.",
        "explanation_es": "“Until stopped” = Continuous; compute se apaga al parar = Production.",
    },
    {
        "exam": 1,
        "id": "q22_delta_sharing_egress_costs",
        "question_es": "Delta Sharing entre AWS (US) y Azure (EU): ¿por qué sube el coste?",
        "options": [
            "Storage costs because Delta Sharing requires replication of data across clouds and regions",
            "Flat rate fee is applied for all shared data by Delta Sharing protocol",
            "DBU costs for running Delta Sharing servers across cloud boundaries",
            "Egress fees are incurred when data transferred between cloud providers and regions",
        ],
        "answer": "Egress fees are incurred when data transferred between cloud providers and regions",
        "explanation_es": "El protocolo es “gratis”, pero la nube cobra egress al cruzar región/proveedor.",
    },
    {
        "exam": 1,
        "id": "q23_sql_warehouse_auto_stop_benefit",
        "question_es": "SQL Warehouse Auto Stop: ¿qué beneficio aporta?",
        "options": [
            "Improves the performance of the warehouse by automatically stopping ideal services",
            "Minimizes the total running time of the warehouse",
            "Increases the availability of the warehouse by automatically stopping long-running SQL queries",
            "Provides higher security by automatically stopping unused ports of the warehouse",
        ],
        "answer": "Minimizes the total running time of the warehouse",
        "explanation_es": "Ahorro de costes: se apaga cuando está idle.",
    },
    {
        "exam": 1,
        "id": "q24_time_travel_missing_files_vacuum",
        "question_es": "Delta Time Travel falla por archivos desaparecidos: ¿qué comando los borró?",
        "options": ["VACUUM", "ZORDER BY", "DELETE", "OPTIMIZE"],
        "answer": "VACUUM",
        "explanation_es": "VACUUM borra físicamente archivos antiguos no referenciados.",
    },
    {
        "exam": 1,
        "id": "q25_delete_keep_salary_logic",
        "question_es": "SQL: quieres quedarte con salarios > 3000. ¿Qué DELETE haces?",
        "options": [
            "UPDATE employees WHERE salary <= 3000 WHEN MATCHED DELETE;",
            "DELETE FROM employees WHERE salary > 3000;",
            "SELECT CASE WHEN salary <= 3000 THEN DELETE ELSE UPDATE END FROM employees;",
            "DELETE FROM employees WHERE salary <= 3000;",
        ],
        "answer": "DELETE FROM employees WHERE salary <= 3000;",
        "explanation_es": "Borras lo que NO quieres conservar (≤ 3000).",
    },
    {
        "exam": 1,
        "id": "q26_databricks_web_app_location",
        "question_es": "¿Dónde vive la app web/UI de Databricks (arquitectura lakehouse)?",
        "options": ["Databricks-managed cluster", "Control plane", "Data plane", "Customer Cloud Account"],
        "answer": "Control plane",
        "explanation_es": "UI/gestión en control plane; cómputo y datos (clásico) en data plane.",
    },
    {
        "exam": 1,
        "id": "q27_medallion_silver_description",
        "question_es": "Medallion: ¿qué describe mejor la capa Silver?",
        "options": [
            "The table structure in this layer resembles that of the source system table structure with any additional metadata columns like the load time, and input file name.",
            "They maintain data that powers analytics, machine learning, and production applications",
            "They maintain raw data ingested from various sources",
            "They provide a more refined view of raw data, where it’s filtered, cleaned, and enriched.",
        ],
        "answer": "They provide a more refined view of raw data, where it’s filtered, cleaned, and enriched.",
        "explanation_es": "Silver = datos limpios/filtrados/enriquecidos (no agregados finales).",
    },
    {
        "exam": 1,
        "id": "q28_autoloader_schema_evolution_fail",
        "question_es": "Auto Loader con schemaEvolutionMode=failOnNewColumns: ¿qué pasa si llega columna nueva?",
        "options": [
            "The stream fails, and all new columns are saved in a rescued data column for later processing.",
            "The stream fails temporarily but continues by ignoring the new columns without schema update.",
            "The stream fails and will not restart unless the schema is manually updated or the problematic data file is removed.",
            "The stream fails, but it automatically restarts after updating the schema with the new columns.",
        ],
        "answer": "The stream fails and will not restart unless the schema is manually updated or the problematic data file is removed.",
        "explanation_es": "Modo estricto: falla y requiere intervención (actualizar schema o quitar archivo).",
    },
    {
        "exam": 1,
        "id": "q29_spot_instances_benefit",
        "question_es": "¿Beneficio principal de usar spot instances en clusters?",
        "options": [
            "Significantly lower compute costs",
            "Increased security and compliance",
            "Guaranteed job execution time",
            "Reduced data storage latency",
        ],
        "answer": "Significantly lower compute costs",
        "explanation_es": "Mucho más barato, pero pueden quitarte la instancia (interrupciones).",
    },
    {
        "exam": 1,
        "id": "q30_create_table_comment_ctas",
        "question_es": "SQL: crear tabla con CTAS y añadir COMMENT. ¿Cuál es correcto?",
        "options": [
            'CREATE TABLE payments AS SELECT * FROM bank_transactions COMMENT "..."',
            'CREATE TABLE payments COMMENT("...") AS SELECT * FROM bank_transactions',
            'COMMENT("...") CREATE TABLE payments AS SELECT * FROM bank_transactions',
            'CREATE TABLE payments COMMENT "This table contains sensitive information" AS SELECT * FROM bank_transactions',
        ],
        "answer": 'CREATE TABLE payments COMMENT "This table contains sensitive information" AS SELECT * FROM bank_transactions',
        "explanation_es": "COMMENT va justo después del nombre de la tabla y antes del AS SELECT.",
    },
    {
        "exam": 1,
        "id": "q31_uc_modify_privilege",
        "question_es": "Unity Catalog: ¿qué permite el privilegio MODIFY en una tabla?",
        "options": [
            "It gives the ability to add, update, or delete data within the table",
            "It gives the ability to modify data in the table",
            "It gives the ability to delete data from the table",
            "It gives the ability to add data from the table",
        ],
        "answer": "It gives the ability to add, update, or delete data within the table",
        "explanation_es": "MODIFY cubre DML: INSERT/UPDATE/DELETE/MERGE/TRUNCATE (contenido, no estructura).",
    },
    {
        "exam": 1,
        "id": "q32_grant_all_privileges_syntax",
        "question_es": "SQL (UC): ¿cómo otorgas permisos completos sobre la tabla employees?",
        "options": [
            "GRANT ALL PRIVILEGES ON TABLE employees TO hr_team",
            "GRANT ALL PRIVILEGES ON employees TO hr_team",
            "GRANT FULL PRIVILEGES ON TABLE employees TO hr_team",
            "GRANT SELECT, MODIFY, CREATE, READ_METADATA ON TABLE employees TO hr_team",
        ],
        "answer": "GRANT ALL PRIVILEGES ON TABLE employees TO hr_team",
        "explanation_es": "Sintaxis esperada: ALL PRIVILEGES y especificar ON TABLE.",
    },
    {
        "exam": 1,
        "id": "q33_data_plane_customer_account",
        "question_es": "Lakehouse: ¿qué componente está en la cuenta cloud del cliente (data plane)?",
        "options": ["Databricks web application", "Cluster virtual machines", "Workflows", "Notebooks"],
        "answer": "Cluster virtual machines",
        "explanation_es": "En clásico, las VMs del cluster viven en tu cuenta (data plane).",
    },
    {
        "exam": 1,
        "id": "q34_uc_data_lineage",
        "question_es": "Trazar origen/flujo entre tablas, queries, notebooks y dashboards: ¿qué es?",
        "options": ["Delta Live Tables DAGs", "Databricks Lakeflow", "Databricks Jobs", "Unity Catalog Data Lineage"],
        "answer": "Unity Catalog Data Lineage",
        "explanation_es": "Lineage global de plataforma (no solo dentro de un pipeline DLT).",
    },
    {
        "exam": 1,
        "id": "q35_job_email_notifications",
        "question_es": "Jobs: ¿cómo notificas por email al equipo cuando termina una ejecución?",
        "options": [
            "Only Job owner can be configured to be notified when the job completes",
            "They can use Job API to programmatically send emails according to each task status",
            "They can configure email notifications settings in the job page",
            "There is no way to notify users when the job completes",
        ],
        "answer": "They can configure email notifications settings in the job page",
        "explanation_es": "Se configura en la UI del Job (Notifications: success/failure/etc.).",
    },
    {
        "exam": 1,
        "id": "q36_autoloader_definition",
        "question_es": "¿Qué describe mejor Auto Loader (cloudFiles)?",
        "options": [
            "Auto loader monitors a source location, in which files accumulate, to identify and ingest only new arriving files with each command run. While the files that have already been ingested in previous runs are skipped.",
            "Auto loader allows cloning a source Delta table to a target destination at a specific version.",
            "Auto loader allows applying Change Data Capture (CDC) feed to update tables based on changes captured in source data.",
            "Auto loader enables efficient insert, update, deletes, and rollback capabilities by adding a storage layer that provides better data reliability to data lakes.",
        ],
        "answer": "Auto loader monitors a source location, in which files accumulate, to identify and ingest only new arriving files with each command run. While the files that have already been ingested in previous runs are skipped.",
        "explanation_es": "Auto Loader detecta e ingiere solo archivos nuevos en una ruta cloud.",
    },
    {
        "exam": 1,
        "id": "q37_liquid_clustering_function",
        "question_es": "Liquid Clustering: ¿para qué sirve principalmente?",
        "options": [
            "To automate the creation of new data pipelines",
            "To improve the speed of network connectivity between nodes",
            "To incrementally optimize data layout for improved query performance",
            "To encrypt data stored in Delta Lake",
        ],
        "answer": "To incrementally optimize data layout for improved query performance",
        "explanation_es": "Optimiza layout de datos para mejorar consultas (data skipping) de forma incremental.",
    },
    {
        "exam": 1,
        "id": "q38_streaming_trigger_processing_time",
        "question_es": "Structured Streaming: micro-batch cada 2 min. ¿Qué va en el hueco?",
        "options": [
            'trigger(once="2 minutes")',
            'trigger("2 minutes")',
            'processingTime("2 minutes")',
            'trigger(processingTime="2 minutes")',
        ],
        "answer": 'trigger(processingTime="2 minutes")',
        "explanation_es": "trigger(processingTime=...) define el intervalo de micro-batches.",
    },
    {
        "exam": 1,
        "id": "q39_autoloader_path_glob_filter",
        "question_es": "Auto Loader: procesar solo *.jpg. ¿Qué opción usarías?",
        "options": ["fileExtension", "cloudFiles.pathGlobFilter", "cloudFiles.fileExtension", "pathGlobFilter"],
        "answer": "pathGlobFilter",
        "explanation_es": "pathGlobFilter es opción estándar Spark para filtrar por patrón (*.jpg).",
    },
    {
        "exam": 1,
        "id": "q40_dlt_expectations_default_retain",
        "question_es": "DLT EXPECT: para que los registros inválidos se conserven y se reporten, ¿qué pones?",
        "options": [
            "ON VIOLATION FAIL UPDATE",
            "There is no need to add ON VIOLATION clause. By default, records violating the constraint will be kept, and reported as invalid in the event log",
            "ON VIOLATION NONE",
            "ON VIOLATION ADD ROW",
        ],
        "answer": "There is no need to add ON VIOLATION clause. By default, records violating the constraint will be kept, and reported as invalid in the event log",
        "explanation_es": "Por defecto EXPECT = retain (se queda y se reporta).",
    },
    {
        "exam": 1,
        "id": "q41_drop_external_table_keeps_files",
        "question_es": "DROP TABLE: si se borra solo metadato y no archivos, ¿por qué?",
        "options": [
            "The table is external",
            "The user running the command has no permission to delete the data files",
            "The table is managed",
            "Delta prevents deleting files less than retention threshold, just to ensure that no long-running operations are still referencing any of the files to be deleted",
        ],
        "answer": "The table is external",
        "explanation_es": "En tablas externas, DROP borra el puntero/metadatos, no los datos en storage.",
    },
    {
        "exam": 1,
        "id": "q42_cron_syntax_jobs",
        "question_es": "Databricks Jobs: ¿qué representa una expresión Cron?",
        "options": [
            "It’s an expression to represent the run timeout of a job",
            "It’s an expression to represent complex job schedule that can be defined programmatically",
            "It’s an expression to represent the retry policy of a job",
            "It’s an expression to represent the maximum concurrent runs of a job",
        ],
        "answer": "It’s an expression to represent complex job schedule that can be defined programmatically",
        "explanation_es": "Cron define schedules complejos (especialmente vía API).",
    },
    {
        "exam": 1,
        "id": "q43_sql_union_no_duplicates",
        "question_es": "SQL: unir dos tablas y quitar duplicados. ¿Qué comando?",
        "options": [
            "SELECT * FROM students_course_1 INTERSECT SELECT * FROM students_course_2",
            "SELECT * FROM students_course_1 CROSS JOIN SELECT * FROM students_course_2",
            "SELECT * FROM students_course_1 UNION SELECT * FROM students_course_2",
            "SELECT * FROM students_course_1 INNER JOIN SELECT * FROM students_course_2",
        ],
        "answer": "SELECT * FROM students_course_1 UNION SELECT * FROM students_course_2",
        "explanation_es": "UNION elimina duplicados (UNION ALL los mantiene).",
    },
    {
        "exam": 1,
        "id": "q44_optimize_zorder_compute_optimized",
        "question_es": "OPTIMIZE + ZORDER: ¿qué tipo de optimización de recursos priorizarías?",
        "options": ["GPU Optimized", "Compute Optimized", "Storage Optimized", "Memory Optimized"],
        "answer": "Compute Optimized",
        "explanation_es": "Es intensivo en CPU (leer, ordenar, shuffle, reescribir Parquet).",
    },
    {
        "exam": 1,
        "id": "q45_delta_sharing_databricks_to_databricks",
        "question_es": "Compartir tablas UC con otra empresa en Databricks (otra nube) sin replicar: ¿qué opción?",
        "options": ["Shallow clone", "Databricks Connect", "Databricks-to-Databricks Delta Sharing", "External schema in Unity Catalog"],
        "answer": "Databricks-to-Databricks Delta Sharing",
        "explanation_es": "Delta Sharing permite compartir “zero-copy” (sin replicación persistente).",
    },
]